{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_sigmoid(x):\n",
    "    # 隐藏层的激活函数\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax_function(a):\n",
    "    # 全连接层转概率分布\n",
    "    c = np.max(a)\n",
    "    expa = np.exp(a - c)\n",
    "    # overflow avoid\n",
    "    sum_exp = np.sum(expa)\n",
    "    y = expa / sum_exp\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(pred,real):\n",
    "    # 就是重点关注 在正确的上面,是不是熵越小\n",
    "    # 还是符合 误差越大,这个数字就越大\n",
    "    # 交叉熵损失函数,用于衡量输入张量和真实张量的差值\n",
    "    delta = 1e-7 # avoid overflow\n",
    "    return -np.sum(real * np.log(pred+delta))\n",
    "\n",
    "def numerical_gradient(func,input_points_array):\n",
    "    # 计算梯度,就是在输入维度的每个维度上求其导数\n",
    "    delta = 1e-4\n",
    "    tmp_shape = input_points_array.shape\n",
    "    # 缓存原始矩阵形状 如 2,3 ! size 返回的是长度\n",
    "    # 扁平化处理\n",
    "    if input_points_array.ndim != 1:\n",
    "        input_points_array = input_points_array.reshape(1,input_points_array.size).squeeze()\n",
    "    \n",
    "    # 扁平化后直接迭代全部,求偏导\n",
    "    grad = np.zeros_like(input_points_array)\n",
    "    for index in range(input_points_array.size):\n",
    "        temp_val = input_points_array[index]\n",
    "        # f(x+h)\n",
    "        input_points_array[index] = temp_val + delta\n",
    "        fv1 = func(input_points_array)\n",
    "\n",
    "        input_points_array[index] = temp_val - delta\n",
    "        fv2 = func(input_points_array)\n",
    "\n",
    "        grad[index] = (fv1 - fv2) / (delta * 2)\n",
    "        input_points_array[index] = temp_val\n",
    "    \n",
    "    # 还原原始维度\n",
    "    input_points_array = input_points_array.reshape(tmp_shape)\n",
    "    grad = grad.reshape(tmp_shape)\n",
    "    return grad\n",
    "\n",
    "def gradient_decent(f,init_x,lr=0.1,step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad # 梯度下降 \n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_initial_std=0.01):\n",
    "        # input layer 输入层\n",
    "        # hidden_size 隐藏层神经元个数\n",
    "        # output 输出神经元个数\n",
    "        # 初始权重,对 初始神经元正则化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_initial_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_initial_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self,input_array):\n",
    "        W1,W2 = self.params[\"W1\"],self.params[\"W2\"]\n",
    "        b1,b2 = self.params[\"b1\"],self.params[\"b2\"]\n",
    "\n",
    "        # input -> a1 -> z1 -> L2\n",
    "        a1 = np.dot(input_array,W1) + b1\n",
    "        z1 = step_sigmoid(a1) # sigmoid 隐藏层激活函数\n",
    "\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax_function(a2) # 全连接层转分布\n",
    "        return y\n",
    "\n",
    "    def loss(self,input_arrays,reals):\n",
    "        preds = self.predict(input_arrays)\n",
    "        return cross_entropy_error(preds,reals)\n",
    "    \n",
    "    def accuracy(self,input_arrays,reals):\n",
    "        y = self.predict(input_arrays)\n",
    "        y = np.argmax(y,axis=1) # --\n",
    "        reals = np.argmax(reals,axis=1) # ==\n",
    "        accuracy = np.sum(y == reals) / float(input_arrays.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,input_arrays,reals):\n",
    "        loss_W = lambda fake : self.loss(input_arrays,reals)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W,self.params['b2'])\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  mnist import load_mnist\n",
    "(x_train, t_train) ,(x_test, t_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "# hyper parameters\n",
    "\n",
    "iter_num = 100\n",
    "tran_size = x_train.shape[0]\n",
    "batch_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(tran_size/batch_size,1)\n",
    "\n",
    "\n",
    "net = TwoLayerNetwork(input_size=784,hidden_size=50,output_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# mask = np.random.choice(10,5) # array([2, 0, 2, 5, 2])\n",
    "# print(np.random.randn(10,4)[mask])\n",
    "\n",
    "for i in range(iter_num):\n",
    "    print(i)\n",
    "    batch = np.random.choice(tran_size,batch_size)\n",
    "    x_batch = x_train[batch]\n",
    "    t_batch = t_train[batch]\n",
    "\n",
    "    grad = net.numerical_gradient(x_batch,t_batch)\n",
    "\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        # grad 是各个参数矩阵的梯度值\n",
    "        net.params[key] -= lr * grad[key]#梯度下降\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = net.accuracy(x_train,t_train)\n",
    "        test_acc = net.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "    \n",
    "    loss = net.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.22222062e-03, -1.52184059e-03,  6.72948419e-03, ...,\n",
       "         8.00174958e-03, -2.39313615e-03, -1.33481390e-02],\n",
       "       [-4.49973982e-03,  1.37269519e-02,  4.06990424e-03, ...,\n",
       "         1.01186751e-02,  1.08493838e-02, -1.97001029e-03],\n",
       "       [ 1.17904155e-02, -4.63902606e-03, -3.33463003e-03, ...,\n",
       "        -1.34637463e-02, -6.20693957e-03, -5.91758782e-03],\n",
       "       ...,\n",
       "       [ 4.64403001e-03, -1.17841175e-02,  1.79572840e-03, ...,\n",
       "        -3.56421296e-03, -8.97899883e-03, -1.25014819e-02],\n",
       "       [ 9.69811792e-03, -1.51452051e-03,  9.83568274e-03, ...,\n",
       "        -7.65879571e-03, -1.31815423e-02, -1.30733235e-02],\n",
       "       [ 1.86015706e-04, -6.23936669e-03, -4.48041228e-03, ...,\n",
       "         1.89045681e-05,  1.92439341e-03, -8.98577422e-04]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.08401433, 0.0626904 , 0.13044008, 0.19866606, 0.05155596,\n",
       "         0.06534478, 0.2074063 , 0.05673433, 0.11087824, 0.03226952]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(np.array([x_test[0]])),np.array([t_test[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
